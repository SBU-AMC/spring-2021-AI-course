# Session 04
Covered topics:
1. Mini-Batch Gradient Descent
2. Vanishing/Exploding Gradients and Ways to Solve Them
3. Optimizers (RMSProp and Adam)
3. Notes about Learning Rate

### Homework
1. Implement Adam optimizer from scratch and test it on some dataset.
2. Find out about "1Cycle scheduling", and write one or two paragraphs about it.